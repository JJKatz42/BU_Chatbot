import re

import langchain.chat_models
import langchain.schema

import src.libs.search.search_agent.schemas as schemas
import src.libs.search.search_agent.utils as utils
import src.libs.logging as logging

logger = logging.getLogger(__name__)

# You are a filter for (and sometimes, a helper to) AI assistant which is trained on a given university's documentation. 
# You will return a response very similar to the one we provide you; this is all you should ever return.
instructions_msg = langchain.prompts.SystemMessagePromptTemplate.from_template(
    template="Ensure that an answer that was generated by an LLM is formatted correctly."
    "Your task is to ensure that the answer is formatted according to **ALL** the following rules.\n"
    "Rule 1. If there is a particularly crucial or salient part of the answer, ensure that it is wrapped in asterisks like **this**.\n"
    "Rule 1.1. NEVER wrap large swaths of the answer or many discontinuous parts of the answer.\n"
    "Rule 1.2. NEVER use single asterisks; there must always be two on both sides of the wrap, not one. (That is, NEVER italicize; only bold.)\n"
    "Rule 2. The answer does not need to begin with '{answer_prefix}'.\n"
    "Rule 2.1. NEVER include 'Answer:' at the very beginning of the answer.\n"
    "Rule 3. If the question requests a list of items, the answer should contain a bulleted list of items.\n"
    "Rule 3.1. The lists should be returned in HTML style (that is, with <ol> or <ul> tags).\n"
    "Rule 3.2. You should NEVER use line breaks in or around lists.\n"
    "Rule 3.3. If you see line breaks surrounding an <ol> or <ul> tag, remove them.\n"
    "Rule 3.4. NEVER bold entire lists: you may bold individual items in the list, but never the entire list. (Remember that bolding entails wrapping something with two asterisks.)\n"
    "Rule 4. If you are including links in your response, make sure they will open in a new tab. You can do this by adding target='_blank' to the link tag.\n"
    "Rule 5. If the answer gives a 'list' of steps that only has one step, it should not be formatted as a list. Instead, it should be formatted as a single paragraph.\n",

    template_format="f-string",
)

    # "Rule 6. NEVER include 'Answer:' at the very beginning of the answer.\n"
    # "Rule 7. If you believe that the question addressed is outside of the scope of university data, you may answer it yourself as a normal GPT query.\n",

# TODO: implement some version of the following, which fail miserably in testing with gpt-3.5-turbo
#   "Rule X. If the original question asks for a specific tone or style, the answer should be written in that tone or "
#   "style.  For example if the question asks 'written in the style of shakespear', please reformat the entire answer "
#   "in the style of shakespear.  This is allowed to break Rule 3, as well as Rule 1 as long as it remains generally "
#   "safe for a workplace environment.\n"


question_msg_prompt_tmpl = langchain.prompts.HumanMessagePromptTemplate.from_template(
    template="Question: {question}", template_format="f-string"
)

generated_answer_msg_prompt_tmpl = langchain.prompts.HumanMessagePromptTemplate.from_template(
    template="Answer: {answer}", template_format="f-string"
)

instructions_reminder_msg = langchain.schema.SystemMessage(
    content="Please re-format the answer based on **ALL** the formatting rules."
)


def build_format_answer_prompt_msgs(
    generated_answer: str,
    sources: list[schemas.SearchResult],
    question: str,
) -> list[langchain.prompts.base.BaseMessage]:
    # Set answer prefix
    # answer_prefix = "Based on the provided sources, " if len(sources) > 1 else "Based on the provided source, "
    answer_prefix = ""
    prompt_messages = [
        instructions_msg.format(answer_prefix=answer_prefix),
        question_msg_prompt_tmpl.format(question=question),
        generated_answer_msg_prompt_tmpl.format(answer=generated_answer),
        instructions_reminder_msg,
    ]

    return prompt_messages


@utils.llm_schema_gen_retry_config
async def format_answer(
    generated_answer: str,
    llm: langchain.chat_models.ChatOpenAI,
    fallback_llm: langchain.chat_models.ChatOpenAI,
    query: str,
    sources: list[schemas.SearchResult],
    max_answer_tokens: int = 500,
):
    """Format the answer and excerpt generated by an LLM.

    Operates in-place on the generated_answer.

    Args:
        generated_answer: The answer and excerpt generated by an LLM.
        llm: The LLM to use for formatting the answer and excerpt.
        fallback_llm: The fallback LLM to use for formatting the answer and excerpt if the primary LLM is not
            appropriate for the prompt.
        query: The query that was used to generate the answer and excerpt.
        sources: The sources that were used to generate the answer and excerpt.
        max_answer_tokens: The maximum number of tokens to use for the primary LLM before using the fallback

    Returns:
        None
    """
    prompt_msgs = build_format_answer_prompt_msgs(
        generated_answer=generated_answer,
        sources=sources,
        question=query,
    )

    llm_to_use = utils.get_llm_to_use(prompt_msgs, llm, fallback_llm, max_answer_tokens)
    llm_answer_response_message = await llm_to_use.apredict_messages(
        messages=prompt_msgs,
        request_timeout=15,
    )

    # Updated the generated answer
    formatted_answer = llm_answer_response_message.content

    return formatted_answer
