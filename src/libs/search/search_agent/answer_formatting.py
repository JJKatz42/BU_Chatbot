import re

import langchain.chat_models
import langchain.schema

import src.libs.search.search_agent.schemas as schemas
import src.libs.search.search_agent.utils as utils
import src.libs.logging as logging

logger = logging.getLogger(__name__)


instructions_msg = langchain.prompts.SystemMessagePromptTemplate.from_template(
    template="You are tasked with ensuring that an answer that was generated by an LLM is formatted correctly. "
    "Your task is to ensure that the answer is formatted according to **ALL** the following rules. If the answer "
    "is already formatted correctly, please leave it as is.\n\n"
    "Rule 1. Ensure that crucial or definitive parts of the answer are wrapped in asterisks like **this**.\n"
    "Rule 2. The answer must start with '{answer_prefix}'.\n"
    "Rule 3. If the question requests a list of items, the answer should contain a bulleted list of items "
    "appropriate to the question.",
    template_format="f-string",
)

# TODO: implement some version of the following, which fail miserably in testing with gpt-3.5-turbo
#   "Rule X. If the original question asks for a specific tone or style, the answer should be written in that tone or "
#   "style.  For example if the question asks 'written in the style of shakespear', please reformat the entire answer "
#   "in the style of shakespear.  This is allowed to break Rule 3, as well as Rule 1 as long as it remains generally "
#   "safe for a workplace environment.\n"


question_msg_prompt_tmpl = langchain.prompts.HumanMessagePromptTemplate.from_template(
    template="Question: {question}", template_format="f-string"
)

generated_answer_msg_prompt_tmpl = langchain.prompts.HumanMessagePromptTemplate.from_template(
    template="Answer: {answer}", template_format="f-string"
)

instructions_reminder_msg = langchain.schema.SystemMessage(
    content="Please re-format the answer based on **ALL** the formatting rules."
)


def build_format_answer_prompt_msgs(
    generated_answer: str,
    sources: list[schemas.SearchResult],
    question: str,
) -> list[langchain.prompts.base.BaseMessage]:
    # Set answer prefix
    answer_prefix = "Based on the provided sources, " if len(sources) > 1 else "Based on the provided source, "

    prompt_messages = [
        instructions_msg.format(answer_prefix=answer_prefix),
        question_msg_prompt_tmpl.format(question=question),
        generated_answer_msg_prompt_tmpl.format(answer=generated_answer),
        instructions_reminder_msg,
    ]

    return prompt_messages


@utils.llm_schema_gen_retry_config
async def format_answer(
    generated_answer: str,
    llm: langchain.chat_models.ChatOpenAI,
    fallback_llm: langchain.chat_models.ChatOpenAI,
    query: str,
    sources: list[schemas.SearchResult],
    max_answer_tokens: int = 500,
):
    """Format the answer and excerpt generated by an LLM.

    Operates in-place on the generated_answer.

    Args:
        generated_answer: The answer and excerpt generated by an LLM.
        llm: The LLM to use for formatting the answer and excerpt.
        fallback_llm: The fallback LLM to use for formatting the answer and excerpt if the primary LLM is not
            appropriate for the prompt.
        query: The query that was used to generate the answer and excerpt.
        sources: The sources that were used to generate the answer and excerpt.
        max_answer_tokens: The maximum number of tokens to use for the primary LLM before using the fallback

    Returns:
        None
    """
    prompt_msgs = build_format_answer_prompt_msgs(
        generated_answer=generated_answer,
        sources=sources,
        question=query,
    )

    llm_to_use = utils.get_llm_to_use(prompt_msgs, llm, fallback_llm, max_answer_tokens)
    llm_answer_response_message = await llm_to_use.apredict_messages(
        messages=prompt_msgs,
        request_timeout=15,
    )

    # Updated the generated answer
    formatted_answer = llm_answer_response_message.content

    return formatted_answer
